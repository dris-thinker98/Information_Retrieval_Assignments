{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR2021_A1_27",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh7N0Z5SbRMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bcea07e-b4fa-4473-8479-d303c889c099"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5e7CzUbStC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90afa15-752b-4689-e509-1d3cc9bc8621"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bjoV8hBbqhN"
      },
      "source": [
        "#nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikV38GaiOqyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010d9f2d-172e-4ff9-8a7f-6f7f7c4d52f9"
      },
      "source": [
        "# Getting path of folder stories and folders inside it\n",
        "\n",
        "getPath = str('/content/drive/MyDrive/stories')\n",
        "print(getPath)\n",
        "getAllFolders = [x[0] for x in os.walk(getPath)]\n",
        "\n",
        "getAllFolders.pop(len(getAllFolders)-1)\n",
        "print(getAllFolders)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/stories\n",
            "['/content/drive/MyDrive/stories', '/content/drive/MyDrive/stories/SRE', '/content/drive/MyDrive/stories/FARNON']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoxtMS7aOiqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7680e5-d690-47da-eafa-8ff074ae4803"
      },
      "source": [
        "#After analysing we found index.html that contains all the file names and titles\n",
        "# create a tuple of all files path and title\n",
        "\n",
        "list_of_all_files = []\n",
        "flag = 0\n",
        "for i in getAllFolders:\n",
        "    path = i+\"/index.html\"\n",
        "    print(path)\n",
        "    openIndexFile = open(path, 'r')\n",
        "    getContent = openIndexFile.read().strip()\n",
        "    openIndexFile.close()\n",
        "    \n",
        "    # file name is enclosed in Anchor tag of html file\n",
        "    getFileName = re.findall('><A HREF=\"(.*)\">', getContent)\n",
        "    #print(getFileName)\n",
        "    \n",
        "    # file title is enclosed in <BR><TD> tag of html file\n",
        "    getFileTitle = re.findall('<BR><TD> (.*)\\n', getContent)\n",
        "    #print(len(getFileTitle))\n",
        "    \n",
        "    #for excluding folder inside stories which are at 0th and 1st index\n",
        "    if flag==0:\n",
        "        getFileName = getFileName[2:]\n",
        "        flag = 1\n",
        "    for j in range(len(getFileName)):\n",
        "        list_of_all_files.append((str(i)+\"/\" + str(getFileName[j]), getFileTitle[j]))\n",
        "    \n",
        "#print(len(list_of_all_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/stories/index.html\n",
            "/content/drive/MyDrive/stories/SRE/index.html\n",
            "/content/drive/MyDrive/stories/FARNON/index.html\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR0jNENAOylv"
      },
      "source": [
        "#Convert data in to lower case\n",
        "def lower(text):\n",
        "    return np.char.lower(text)\n",
        "\n",
        "# remove all the stopwords(‘off’, ‘is’, ‘s’, ‘am’, ‘or’, ‘who’, e.t.c) from data\n",
        "def stopWords(text):\n",
        "    stop_words = stopwords.words('english')\n",
        "    words = word_tokenize(str(text))\n",
        "    new_text = \"\"\n",
        "    for w in words:\n",
        "        if w not in stop_words:\n",
        "            new_text = new_text + \" \" + w\n",
        "    return np.char.strip(new_text)\n",
        "\n",
        "# remove all the punctuation symbols\n",
        "def punctuationSymbols(text):\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in range(len(symbols)):\n",
        "        text = np.char.replace(text, symbols[i], ' ')\n",
        "        text = np.char.replace(text, \"  \", \" \")\n",
        "        text = np.char.replace(text, ',', '')\n",
        "    return text\n",
        "\n",
        "# remove all apostrophes  \n",
        "def unUsed(text):\n",
        "    return np.char.replace(text, \"'\", \"\")\n",
        "\n",
        "# remove all the one length words\n",
        "def lengthOneChar(text):\n",
        "    words = word_tokenize(str(text))\n",
        "    new_text = \"\"\n",
        "    for j in words:\n",
        "        if len(j) > 1:\n",
        "            new_text = new_text + \" \" + j\n",
        "    return np.char.strip(new_text)\n",
        "\n",
        "# replace numeric values to their coreesponding text value\n",
        "def deleteNumerics(text):\n",
        "    text = np.char.replace(text, \"0\", \" zero \")\n",
        "    text = np.char.replace(text, \"1\", \" one \")\n",
        "    text = np.char.replace(text, \"2\", \" two \")\n",
        "    text = np.char.replace(text, \"3\", \" three \")\n",
        "    text = np.char.replace(text, \"4\", \" four \")\n",
        "    text = np.char.replace(text, \"5\", \" five \")\n",
        "    text = np.char.replace(text, \"6\", \" six \")\n",
        "    text = np.char.replace(text, \"7\", \" seven \")\n",
        "    text = np.char.replace(text, \"8\", \" eight \")\n",
        "    text = np.char.replace(text, \"9\", \" nine \")\n",
        "    return text\n",
        "\n",
        "#perform stemming\n",
        "def performStemming(text):\n",
        "    stemmer= PorterStemmer()\n",
        "    \n",
        "    tokens = word_tokenize(str(text))\n",
        "    new_text = \"\"\n",
        "    for j in tokens:\n",
        "        new_text = new_text + \" \" + stemmer.stem(j)\n",
        "    return np.char.strip(new_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgpSIKUHc11D"
      },
      "source": [
        "#call all the above functions to preprocess the data\n",
        "def preprocess(data, query):    \n",
        "    data = lower(data)\n",
        "    data = stopWords(data)\n",
        "    data = punctuationSymbols(data) #remove comma seperately\n",
        "    data = unUsed(data)\n",
        "    data = deleteNumerics(data)\n",
        "    data = lengthOneChar(data)\n",
        "    data = performStemming(data)\n",
        "    # data = lemma(data)\n",
        "    return data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vzApyAPdGG9"
      },
      "source": [
        "num = 0\n",
        "listsOfDocs = pd.DataFrame()\n",
        "for i in list_of_all_files:\n",
        "    #for reading files with all the extensions including .txt\n",
        "  with open(i[0], 'r', encoding = 'utf-8', errors = 'ignore') as f2:\n",
        "    text = f2.read().strip()\n",
        "    f2.close()\n",
        "    #call preprocess for each file \n",
        "    preprocessed_text = preprocess(text, False)\n",
        "    js = word_tokenize(str(preprocessed_text))\n",
        "\n",
        "    #making posting lists for words\n",
        "    for j in js:\n",
        "      if j in listsOfDocs:\n",
        "        p = listsOfDocs[j][0]\n",
        "        p.add(num)\n",
        "        listsOfDocs[j][0] = p\n",
        "      else:\n",
        "        listsOfDocs.insert(value=[{num}], loc=0, column=j)\n",
        "    num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THOWQQ2DUfLT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEgdrQPpMeg6"
      },
      "source": [
        "#converting dataframe postings to dictionary\n",
        "df = {}\n",
        "for i in listsOfDocs:\n",
        "  df[i] = listsOfDocs[i][0]\n",
        "\n",
        "#df['thought']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPcMzzNSE8r"
      },
      "source": [
        "#Creating set for all documents\n",
        "totals = set()\n",
        "for i in range(467):\n",
        "  totals.add(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DamEjfT0TIiW"
      },
      "source": [
        "# this will pick up two words and corresponding operator respectively\n",
        "def evaluateQuery(data,query):\n",
        "  com = 0\n",
        "  print(data)\n",
        "  print(query)\n",
        "  while len(data)>1:\n",
        "    first = data.pop(0)\n",
        "    second = data.pop(0)\n",
        "    operator = query.pop(0)\n",
        "    print(first,operator,second)\n",
        "    # if OR operator calling applyOr function\n",
        "    if (operator == 'OR'):\n",
        "      x,y = applyOr(first,second)\n",
        "      com += x\n",
        "      data.insert(0,'calculated')\n",
        "      df['calculated'] = y\n",
        "    # if AND operator calling applyAnd function\n",
        "    elif (operator == 'AND'):\n",
        "      x,y = applyAnd(first,second)\n",
        "      com += x\n",
        "      data.insert(0,'calculated')\n",
        "      df['calculated'] = y\n",
        "    # if AND operator calling applyNotAnd function\n",
        "    elif (operator == 'AND NOT'):\n",
        "      x,y = applyNotAnd(first,second)\n",
        "      com += x\n",
        "      data.insert(0,'calculated')\n",
        "      df['calculated'] = y\n",
        "    # if OR operator calling applyNotOr function\n",
        "    elif (operator == 'OR NOT'):\n",
        "      x,y = applyNotOr(first,second)\n",
        "      com += x\n",
        "      data.insert(0,'calculated')\n",
        "      df['calculated'] = y\n",
        "\n",
        "  #Printing result\n",
        "  print('Number of documents matched: ',len(df['calculated']))\n",
        "  print('No. of comparisons required: ',com)\n",
        "\n",
        "\n",
        "#calculate Or (union)\n",
        "def applyOr(first,second):\n",
        "  Or = []\n",
        "  firstPosting = list(df[first])\n",
        "  secondPosting = list(df[second])\n",
        "  i=0\n",
        "  j=0\n",
        "  comparison = 0\n",
        "  while i<len(firstPosting) or j<len(secondPosting):\n",
        "    if i<len(firstPosting) and j<len(secondPosting) and (firstPosting[i]==secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    elif i < len(firstPosting)and j<len(secondPosting) and (firstPosting[i]<secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "    elif i < len(firstPosting)and j<len(secondPosting) and (firstPosting[i]>secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(secondPosting[j])\n",
        "      j+=1\n",
        "    elif i < len(firstPosting):\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "    elif j<len(secondPosting):\n",
        "      Or.append(secondPosting[j])\n",
        "      j+=1\n",
        "\n",
        "  return comparison,set(Or)\n",
        "\n",
        "#Calculate And(Intersection)\n",
        "def applyAnd(first,second):\n",
        "  And = []\n",
        "  firstPosting = list(df[first])\n",
        "  secondPosting = list(df[second])\n",
        "  i=0\n",
        "  j=0\n",
        "  comparison = 0\n",
        "  while i<len(firstPosting) and j<len(secondPosting):\n",
        "    if (firstPosting[i]==secondPosting[j]):\n",
        "      And.append(firstPosting[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    elif (firstPosting[i]<secondPosting[j]):\n",
        "      i+=1\n",
        "    else:\n",
        "      j+=1\n",
        "    comparison+=1\n",
        "  return comparison,set(And)\n",
        "\n",
        "#Calculate Not\n",
        "def notOperation(ls):\n",
        "  temp = set()\n",
        "  for i in totals:\n",
        "    if i not in ls:\n",
        "      temp.add(i)\n",
        "  return temp\n",
        "\n",
        "#Calculate Not Or\n",
        "def applyNotOr(first,second):\n",
        "  Or = []\n",
        "  firstPosting = list(df[first])\n",
        "  secondPosting = df[second]\n",
        "  secondPosting = list(notOperation(secondPosting))\n",
        "  i=0\n",
        "  j=0\n",
        "  comparison = 0\n",
        "  while i<len(firstPosting) or j<len(secondPosting):\n",
        "    if i<len(firstPosting) and j<len(secondPosting) and (firstPosting[i]==secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    elif i < len(firstPosting)and j<len(secondPosting) and (firstPosting[i]<secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "    elif i < len(firstPosting)and j<len(secondPosting) and (firstPosting[i]>secondPosting[j]):\n",
        "      comparison+=1\n",
        "      Or.append(secondPosting[j])\n",
        "      j+=1\n",
        "    elif i < len(firstPosting):\n",
        "      Or.append(firstPosting[i])\n",
        "      i+=1\n",
        "    elif j<len(secondPosting):\n",
        "      Or.append(secondPosting[j])\n",
        "      j+=1\n",
        "\n",
        "  return comparison,set(Or)\n",
        "\n",
        "\n",
        "#Calculate Not And\n",
        "def applyNotAnd(first,second):\n",
        "  And = []\n",
        "  firstPosting = list(df[first])\n",
        "  secondPosting = df[second]\n",
        "  secondPosting = list(notOperation(secondPosting))\n",
        "  i=0\n",
        "  j=0\n",
        "  comparison = 0\n",
        "  while i<len(firstPosting) and j<len(secondPosting):\n",
        "    if (firstPosting[i]==secondPosting[j]):\n",
        "      And.append(firstPosting[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    elif (firstPosting[i]<secondPosting[j]):\n",
        "      i+=1\n",
        "    else:\n",
        "      j+=1\n",
        "    comparison+=1\n",
        "  return comparison,set(And)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB2NUH9xuPb_"
      },
      "source": [
        "#Check if query asked is valid to our documents\n",
        "def check_data(data):\n",
        "  for i in data:\n",
        "    if i not in df:\n",
        "      return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5nFRASRRjSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3459bd6e-826b-471d-cc46-5d9bf31a5a7e"
      },
      "source": [
        "#taking input and preprocessing it\r\n",
        "inp = input(\"Enter input : \")\r\n",
        "data = preprocess(inp,False)\r\n",
        "data = data.item(0).split()\r\n",
        "if (check_data(data)==0):\r\n",
        "  print('Invalid Query')\r\n",
        "else:\r\n",
        "  #print(type(data))\r\n",
        "  #print(len(data))\r\n",
        "  query = input(\"Query : \") \r\n",
        "  query = query.split(',')\r\n",
        "  queryList = []\r\n",
        "  for i in range(len(query)):\r\n",
        "    p = query.pop()\r\n",
        "    p = p.replace('[',\"\")\r\n",
        "    p = p.replace(']',\"\")\r\n",
        "    p = p.strip()\r\n",
        "    queryList.append(p)\r\n",
        "  evaluateQuery(data,queryList)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter input : lion stood thoughtfully for a moment\n",
            "<class 'list'>\n",
            "4\n",
            "Query : [ OR, OR , OR ]\n",
            "['lion', 'stood', 'thought', 'moment']\n",
            "['OR', 'OR', 'OR']\n",
            "lion OR stood\n",
            "calculated OR thought\n",
            "calculated OR moment\n",
            "Number of documents matched:  367\n",
            "No. of comparisons required:  935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMJJ0vr8Jk_H"
      },
      "source": [
        "\n",
        "# if takeInput()==1: \n",
        "#   evaluateQuery(data,queryList)\n",
        "# else:\n",
        "#   print('Invalid Query')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip0WOJxDt28Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}